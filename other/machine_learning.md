# ディープラーニング
* 損失関数の出力は多くの場合ひとつのスカラ値

# Generative AI
大規模言語モデルを理解するための基礎知識

1. **機械学習の基礎**：教師あり学習、教師なし学習、深層学習など、基本的な概念を知ることが重要です。
- どのようにしてモデルがデータから学習するのか、全体的なフレームワークを提供します。

2. **ニューラルネットワーク**：大規模言語モデルは深層のニューラルネットワークをベースにしているため、アーキテクチャ、活性化関数、誤差逆伝播法などの基本的な概念を理解する必要があります。
- 具体的なモデルの構造や学習の仕組みを示すものです。この中での概念や技術が、言語モデルのベースとなります。

3. **自然言語処理 (NLP)**：トークン化、埋め込み、シーケンスモデリングなど、テキストデータを処理するための手法や考え方を学ぶことが役立ちます。
- テキストデータの特性や処理方法に関する知識を提供し、ニューラルネットワークをテキストに適用する方法を教えてくれます。

4. **トランスフォーマーアーキテクチャ**：GPTやBERTなどの最新の言語モデルはトランスフォーマーをベースにしています。アテンション機構や位置エンコーディングなどのコンセプトを把握することが重要です。
- 現代の大規模言語モデルの主要な技術的な進化を示すもので、テキストの文脈を理解するためのアーキテクチャとして非常に重要です。

5. **最適化手法**：勾配降下法やアダムなど、ニューラルネットワークの学習を効果的に進めるためのアルゴリズムや技術について知っておくと良いでしょう。
- モデルの学習をスムーズに進めるための技術や手法に関する知識を提供します。

# Transformerアーキテクチャ

1. **自己注意（Self-Attention）メカニズム**：Transformerの核心部分であり、入力シーケンス内の各要素が他の要素とどれだけ関連しているかを計算するメカニズムです。

2. **埋め込み（Embedding）**：単語やトークンを固定長のベクトルに変換する方法。Transformerでは、ポジションエンコーディングと組み合わせて、位置情報を埋め込むことも行われます。

3. **フィードフォワードニューラルネットワーク**：Transformer内部には、自己注意の後に続く線形変換の部分として使用されます。

4. **層正規化（Layer Normalization）**：Transformerの各層の出力を正規化し、学習を安定化させるためのテクニック。

5. **多頭注意（Multi-Head Attention）**：自己注意メカニズムを複数の"頭"で平行に計算し、異なる表現空間での注意を捉えるテクニック。

6. **位置エンコーディング（Positional Encoding）**：Transformerは畳み込みや再帰を使用しないので、シーケンス内のトークンの位置情報を埋め込むための手法が必要です。

7. **残差接続（Residual Connection）**：層の入力をその出力に追加することで、勾配消失や勾配爆発を防ぎ、深いネットワークでの学習を助ける技術。

8. **スケーリング**：特に自己注意の際に、計算されるドット積の値をトークンの次元の平方根でスケーリングすることで、学習の安定性を向上させる。
